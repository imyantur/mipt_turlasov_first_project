{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K3JMV0qwmA_q",
   "metadata": {
    "id": "K3JMV0qwmA_q"
   },
   "outputs": [],
   "source": [
    "! pip install -q schedule pytest # установка библиотек, если ещё не"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "# Библиотеки, которые могут вам понадобиться\n",
    "# При необходимости расширяйте список\n",
    "import time\n",
    "import requests\n",
    "import schedule\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "def get_book_data(book_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Извлекает данные о книге с указанной страницы сайта Books to Scrape.\n",
    "    \n",
    "    Args:\n",
    "        book_url (str): URL страницы книги для парсинга.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Словарь с данными о книге, включающий:\n",
    "            - title: название книги\n",
    "            - price: цена\n",
    "            - rating: рейтинг (количество звезд)\n",
    "            - availability: количество в наличии\n",
    "            - description: описание книги\n",
    "            - product_info: словарь с дополнительной информацией из таблицы\n",
    "              Product Information (UPC, Product Type, Price (excl. tax) и т.д.)\n",
    "    \"\"\"\n",
    "    response = requests.get(book_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Название книги\n",
    "    title = soup.find('h1').text.strip()\n",
    "    \n",
    "    # Цена\n",
    "    price = soup.find('p', class_='price_color').text.strip()\n",
    "    \n",
    "    # Рейтинг (количество звезд)\n",
    "    rating_classes = soup.find('p', class_='star-rating')['class']\n",
    "    rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "    rating = 0\n",
    "    for class_name in rating_classes:\n",
    "        if class_name in rating_map:\n",
    "            rating = rating_map[class_name]\n",
    "            break\n",
    "    \n",
    "    # Количество в наличии\n",
    "    availability = soup.find('p', class_='instock availability').text.strip()\n",
    "    \n",
    "    # Описание\n",
    "    product_description = soup.find('div', id='product_description')\n",
    "    description = ''\n",
    "    if product_description:\n",
    "        description_tag = product_description.find_next_sibling('p')\n",
    "        if description_tag:\n",
    "            description = description_tag.text.strip()\n",
    "    \n",
    "    # Дополнительная информация из таблицы Product Information\n",
    "    product_info = {}\n",
    "    table = soup.find('table', class_='table table-striped')\n",
    "    if table:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            th = row.find('th')\n",
    "            td = row.find('td')\n",
    "            if th and td:\n",
    "                key = th.text.strip()\n",
    "                value = td.text.strip()\n",
    "                product_info[key] = value\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'rating': rating,\n",
    "        'availability': availability,\n",
    "        'description': description,\n",
    "        'product_info': product_info,\n",
    "        'url': book_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [],
   "source": [
    "# Используйте для самопроверки\n",
    "book_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "get_book_data(book_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "def scrape_books(is_save: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Собирает данные обо всех книгах со всех страниц каталога Books to Scrape.\n",
    "    \n",
    "    Проходит по всем страницам каталога (page-1.html, page-2.html и т.д.),\n",
    "    находит ссылки на книги и извлекает данные о каждой книге.\n",
    "    \n",
    "    Args:\n",
    "        is_save (bool): Если True, сохраняет результаты в файл books_data.txt.\n",
    "                        По умолчанию False.\n",
    "        \n",
    "    Returns:\n",
    "        list: Список словарей с данными о всех книгах.\n",
    "    \"\"\"\n",
    "    base_url = 'http://books.toscrape.com/catalogue/'\n",
    "    all_books = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        if page == 1:\n",
    "            page_url = f'{base_url}index.html'\n",
    "        else:\n",
    "            page_url = f'{base_url}page-{page}.html'\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(page_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Находим все ссылки на книги на странице\n",
    "            books_on_page = soup.find_all('article', class_='product_pod')\n",
    "            \n",
    "            if not books_on_page:\n",
    "                break\n",
    "            \n",
    "            for book_article in books_on_page:\n",
    "                # Находим ссылку на страницу книги\n",
    "                link_tag = book_article.find('h3').find('a')\n",
    "                if link_tag:\n",
    "                    book_relative_url = link_tag['href']\n",
    "                    # Убираем относительный путь, если он есть\n",
    "                    if book_relative_url.startswith('../../../'):\n",
    "                        book_relative_url = book_relative_url.replace('../../../', '')\n",
    "                    elif book_relative_url.startswith('../../'):\n",
    "                        book_relative_url = book_relative_url.replace('../../', '')\n",
    "                    elif book_relative_url.startswith('../'):\n",
    "                        book_relative_url = book_relative_url.replace('../', '')\n",
    "                    \n",
    "                    book_full_url = f'{base_url}{book_relative_url}'\n",
    "                    \n",
    "                    try:\n",
    "                        book_data = get_book_data(book_full_url)\n",
    "                        all_books.append(book_data)\n",
    "                        print(f\"Собрана книга: {book_data['title']}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Ошибка при парсинге книги {book_full_url}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(0.5)  # Небольшая задержка, чтобы не перегружать сервер\n",
    "            \n",
    "        except requests.exceptions.HTTPError:\n",
    "            # Если страница не найдена, значит мы дошли до конца\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке страницы {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    if is_save:\n",
    "        with open('books_data.txt', 'w', encoding='utf-8') as f:\n",
    "            for book in all_books:\n",
    "                f.write(f\"Название: {book['title']}\\n\")\n",
    "                f.write(f\"Цена: {book['price']}\\n\")\n",
    "                f.write(f\"Рейтинг: {book['rating']} звезд\\n\")\n",
    "                f.write(f\"В наличии: {book['availability']}\\n\")\n",
    "                f.write(f\"Описание: {book['description']}\\n\")\n",
    "                f.write(f\"URL: {book['url']}\\n\")\n",
    "                f.write(\"Дополнительная информация:\\n\")\n",
    "                for key, value in book['product_info'].items():\n",
    "                    f.write(f\"  {key}: {value}\\n\")\n",
    "                f.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        print(f\"\\nДанные сохранены в файл books_data.txt\")\n",
    "    \n",
    "    return all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [],
   "source": [
    "# Проверка работоспособности функции\n",
    "res = scrape_books(is_save=True) # Допишите ваши аргументы\n",
    "print(type(res), len(res)) # и проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [],
   "source": [
    "def run_scheduled_scraping():\n",
    "    \"\"\"\n",
    "    Запускает функцию сбора данных с сохранением в файл.\n",
    "    \"\"\"\n",
    "    print(f\"Запуск сбора данных в {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    scrape_books(is_save=True)\n",
    "    print(f\"Сбор данных завершен в {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# Настройка расписания: каждый день в 19:00\n",
    "schedule.every().day.at(\"19:00\").do(run_scheduled_scraping)\n",
    "\n",
    "print(\"Расписание настроено: сбор данных будет запускаться каждый день в 19:00\")\n",
    "print(\"Для тестирования можно изменить время на ближайшее (например, '19:05')\")\n",
    "\n",
    "# Бесконечный цикл для проверки расписания\n",
    "# Для тестирования можно использовать ближайшее время\n",
    "# schedule.every().day.at(\"19:05\").do(run_scheduled_scraping)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)  # Проверяем расписание каждую минуту"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "! pytest tests/test_scraper.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}